{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXfiliK8ggW0"
      },
      "source": [
        "# Evaluation of models trained on CheXpert dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-kfWkSGQdMYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7924e7-16da-45db-f6c5-35f71e913611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m174.1/179.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Installing collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-7ZesHH0_g_E"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UlyBZp_Lg22D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from utils import *\n",
        "from parameters import *\n",
        "from train_or_test import *\n",
        "from push_prot_chex import *\n",
        "import cv2 as cv\n",
        "\n",
        "import scipy.stats as st\n",
        "\n",
        "seed = 12\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dMx2iY8UlrGq"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JbYuoRqChtKH"
      },
      "outputs": [],
      "source": [
        "prototype_shape = (20, 128, 1, 1)\n",
        "num_classes = 2\n",
        "\n",
        "normalize = transforms.Normalize(mean=mean,\n",
        "                                 std=std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UnltxUbB48m"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZZmVw6rPAxV"
      },
      "source": [
        "## Pleural effusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H4T64o_oeUHA"
      },
      "outputs": [],
      "source": [
        "data_path = 'effusion/'\n",
        "train_dir = data_path + 'train/'\n",
        "test_dir = data_path + 'test/'\n",
        "train_push_dir = data_path + 'push/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPLX42oxB63_"
      },
      "source": [
        "**Centralized Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g3YB4i9JECM"
      },
      "source": [
        "Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32_e26QMefPv"
      },
      "outputs": [],
      "source": [
        "# train set\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    train_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=False)\n",
        "# push set\n",
        "train_push_dataset = datasets.ImageFolder(\n",
        "    train_push_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]))\n",
        "train_push_loader = torch.utils.data.DataLoader(\n",
        "    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
        "    num_workers=2, pin_memory=False)\n",
        "# test set\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    test_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
        "    num_workers=2, pin_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JMNaSg9JECN"
      },
      "source": [
        "Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCwkEgfgpBcK"
      },
      "outputs": [],
      "source": [
        "model_path = 'Pleural_effusion/' # a path to a folder with the trained models\n",
        "model = torch.load(model_path + 'ppnet_chest/21nopush0.7591.pth')\n",
        "model_1 = torch.load(model_path + 'ppnet_chest_1/21nopush0.7582.pth')\n",
        "model_2 = torch.load(model_path + 'ppnet_chest_2/21nopush0.7164.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C99OENUyuzW3",
        "outputId": "38929c07-306e-4dd3-bbf9-49b37af64b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torcheval/metrics/functional/classification/accuracy.py:275: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:230.)\n",
            "  num_correct = mask.new_zeros(num_classes).scatter_(0, target, mask, reduce=\"add\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.81393469123828 +- 0.02176471836194566 \n",
            "Average specificity:  0.7050388771079471 +- 0.03519407249986154 \n",
            "Average balanced accuracy:  0.7594867841731135 +- 0.006761590097009268\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model, model_1, model_2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loader, class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Kl_AP0DGeP"
      },
      "source": [
        "**Unbiased Local Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGCy-eVAJECO"
      },
      "source": [
        "Distribute the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "N_4mLPjO_bBY"
      },
      "outputs": [],
      "source": [
        "dir_names = os.listdir(train_dir)\n",
        "for client in range(4): # 4 clients\n",
        "  os.mkdir(f'client_{client}')\n",
        "  os.mkdir(f'client_{client}/' + 'train/')\n",
        "  os.mkdir(f'client_{client}/' + 'push/')\n",
        "  os.mkdir(f'client_{client}/' + 'test/')\n",
        "  for class_name in dir_names:\n",
        "    os.mkdir(f'client_{client}/'+ 'train/' + class_name)\n",
        "    os.mkdir(f'client_{client}/'+ 'push/' + class_name)\n",
        "    os.mkdir(f'client_{client}/'+ 'test/' + class_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6qo8Id4q_sBM"
      },
      "outputs": [],
      "source": [
        "distribute_data(train_dir, seed, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Wy52DNJS_u9a"
      },
      "outputs": [],
      "source": [
        "distribute_data(train_push_dir, seed, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hiPb5GJK_xWO"
      },
      "outputs": [],
      "source": [
        "distribute_data(test_dir, seed, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EMdm9-q2M8YF"
      },
      "outputs": [],
      "source": [
        "num_clients = 4\n",
        "train_datasets, train_loaders = [],[]\n",
        "train_push_datasets, train_push_loaders = [],[]\n",
        "test_datasets, test_loaders = [],[]\n",
        "\n",
        "for client in range(num_clients):\n",
        "  # train set\n",
        "  train_dir = f'client_{client}/' + 'train/'\n",
        "  train_push_dir = f'client_{client}/' + 'push/'\n",
        "  test_dir = f'client_{client}/' + 'test/'\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(\n",
        "      train_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ]))\n",
        "  train_datasets.append(train_dataset)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  train_loaders.append(train_loader)\n",
        "\n",
        "  # push set\n",
        "  train_push_dataset = datasets.ImageFolder(\n",
        "      train_push_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "      ]))\n",
        "  train_push_datasets.append(train_push_dataset)\n",
        "\n",
        "  train_push_loader = torch.utils.data.DataLoader(\n",
        "      train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  train_push_loaders.append(train_push_loader)\n",
        "\n",
        "  # test set\n",
        "  test_dataset = datasets.ImageFolder(\n",
        "      test_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ]))\n",
        "  test_datasets.append(test_dataset)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset, batch_size=test_batch_size, shuffle=False,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  test_loaders.append(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDMefiSoJECQ"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-te_Rvl9AhJM"
      },
      "outputs": [],
      "source": [
        "model_1 = torch.load('Pleural_effusion/Local_1/20_11push0.7983.pth')\n",
        "model_2 = torch.load('Pleural_effusion/Local_2/20_11push0.7440.pth')\n",
        "model_3 = torch.load('Pleural_effusion/Local_3/20_11push0.7386.pth')\n",
        "model_4 = torch.load('Pleural_effusion/Local_4/20_11push0.6809.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQiBfcDfJECQ"
      },
      "source": [
        "Evaluate on each unbiased test set and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I7zCLQGRxxz",
        "outputId": "eb338d93-29db-439e-ebd0-a42558454ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6862864077669903 +- 0.06219353335235532 \n",
            "Average specificity:  0.7269054878048781 +- 0.03606364431089226 \n",
            "Average balanced accuracy:  0.7065959477859343 +- 0.02401147217814353\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3, model_4):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(4):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBjL824PF35i"
      },
      "source": [
        "**Unbiased Personalized Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQsCqpglJECU"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ER0ruNtfkHn"
      },
      "outputs": [],
      "source": [
        "model_path = 'Pleural_effusion/Global_good/ppnet_chest/'\n",
        "model_1 = torch.load(model_path + 'client_0_last_round_2_push0.6000.pth').module.to(device)\n",
        "model_2 = torch.load(model_path + 'client_1_last_round_2_push0.6090.pth').module.to(device)\n",
        "model_3 = torch.load(model_path + 'client_2_last_round_2_push0.6552.pth').module.to(device)\n",
        "model_4 = torch.load(model_path + 'client_3_last_round_2_push0.6149.pth').module.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9dQsF2HJECa"
      },
      "source": [
        "Evaluate on each unbiased test set and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNedWSNufe39",
        "outputId": "a9381530-12dd-4ba7-ff74-6e0d83fac66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6915048543689319 +- 0.061672937582765264 \n",
            "Average specificity:  0.5837878787878787 +- 0.024722983139516243 \n",
            "Average balanced accuracy:  0.6376463665784055 +- 0.020076489232963117\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3, model_4):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(4):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ5z5ovUCpbC"
      },
      "source": [
        "**Unbiased Global Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FZr0NsSJECa"
      },
      "source": [
        "Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVmyCFAe-O6i"
      },
      "outputs": [],
      "source": [
        "model_path = 'Pleural_effusion/Global_good/Fully_global/'\n",
        "model_ser = torch.load(model_path + 'ppnet_chest/server_final_round_2_0.7381.pth').module.to(device)\n",
        "model_ser1 = torch.load(model_path + 'ppnet_chest_1/server_final_round_2_0.6321.pth').module.to(device)\n",
        "model_ser2 = torch.load(model_path + 'ppnet_chest_2/server_final_round_2_0.7448.pth').module.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVGqPFUPxvAy",
        "outputId": "60f4c9bf-662f-49e8-9276-4ccf4d8c9d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.8375404530744337 +- 0.08181752062309355 \n",
            "Average specificity:  0.6440404040404041 +- 0.11739929136454207 \n",
            "Average balanced accuracy:  0.7407904285574188 +- 0.022415569104705057\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_ser, model_ser1, model_ser2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loader, class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsdsNGAsFXlG"
      },
      "source": [
        "**Biased Local model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YROvA3IJECQ"
      },
      "source": [
        "Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6CuPlVwJECR"
      },
      "outputs": [],
      "source": [
        "# train set\n",
        "train_dir = f'drains/' + 'train/'\n",
        "train_push_dir = f'drains/' + 'push/'\n",
        "test_dir = f'drains/' + 'test/'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    train_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "train_datasets[3] = train_dataset\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=False)\n",
        "train_loaders[3] = train_loader\n",
        "\n",
        "# push set\n",
        "train_push_dataset = datasets.ImageFolder(\n",
        "    train_push_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]))\n",
        "train_push_datasets[3] = train_push_dataset\n",
        "\n",
        "train_push_loader = torch.utils.data.DataLoader(\n",
        "    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
        "    num_workers=2, pin_memory=False)\n",
        "train_push_loaders[3] = train_push_loader\n",
        "\n",
        "# test set\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    test_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "test_datasets[3] = test_dataset\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
        "    num_workers=2, pin_memory=False)\n",
        "test_loaders[3] = test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPmRqZ6JECR"
      },
      "source": [
        "Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnRQfsvpwgmz"
      },
      "outputs": [],
      "source": [
        "model_path = 'Pleural_effusion/Local_drain/ppnet_chest/'\n",
        "model_b = torch.load(model_path + '21nopush0.7959.pth')\n",
        "model_b1 = torch.load(model_path + '211nopush0.7959.pth')\n",
        "model_b2 = torch.load(model_path + '21nopush0.7347.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzwqc18JECR"
      },
      "source": [
        "Evaluate on a biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtfVvkoNv0_i",
        "outputId": "16f6bf9c-dc91-4a52-f2db-d47c86c4e87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.5087719298245613 +- 0.008771929824561394 \n",
            "Average specificity:  0.9555555555555554 +- 0.014698618394803265 \n",
            "Average balanced accuracy:  0.7321637426900586 +- 0.011582970829109494\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_b, model_b1, model_b2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG4zGYTGJECS"
      },
      "source": [
        "Evaluate on unbiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrCnbec0v66_",
        "outputId": "49b821e7-98bc-4ca6-f17c-8a7cd69c20f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.05221143473570658 +- 0.0158205027208791 \n",
            "Average specificity:  0.9551515151515152 +- 0.009001430201485248 \n",
            "Average balanced accuracy:  0.5036814749436108 +- 0.003759958209356777\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_b, model_b1, model_b2):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LeOG-QwLySv"
      },
      "source": [
        "**Biased Personalized Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2HswVl_JECh"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrQAEJ-7L2IF"
      },
      "outputs": [],
      "source": [
        "# unbiased clients\n",
        "model_path = 'Pleural_effusion/Global_biased/ppnet_chest/'\n",
        "model_1 = torch.load(model_path + 'client_0_last_round_2_push0.5642.pth')\n",
        "model_2 = torch.load(model_path + 'client_1 last_round_2_push0.6276.pth')\n",
        "model_3 = torch.load(model_path + 'client_2 last_round_2_push0.6269.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Ar25-MzBFC"
      },
      "outputs": [],
      "source": [
        "# biased client, results for three runs\n",
        "model_b = torch.load('Pleural_effusion/Global_biased/ppnet_chest/client_3_last_round_2_push0.8063.pth').module.to(device)\n",
        "model_b1 = torch.load('Pleural_effusion/Global_biased/ppnet_chest_1/client_3 last_round_2_push0.8429.pth').module.to(device)\n",
        "model_b2 = torch.load('Pleural_effusion/Global_biased/ppnet_chest_2/client_3_last_round_2_push0.8325.pth').module.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECOMoZwQJECh"
      },
      "source": [
        "Evaluate biased model on a biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM21muhPziTT",
        "outputId": "e28a67f2-cc7d-4bfe-95db-7cbb87b8e00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.3684210526315789 +- 0.0303868562731382 \n",
            "Average specificity:  0.9277777777777777 +- 0.0388888888888889 \n",
            "Average balanced accuracy:  0.6480994152046784 +- 0.009860566508138725\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_b, model_b1, model_b2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsvcnsZmJECi"
      },
      "source": [
        "Evaluate biased model on unbiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMubUlIVzyrz",
        "outputId": "0fddb9f6-a92e-40e0-b68c-b8360fb4e5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.07292340884573895 +- 0.023127525785966423 \n",
            "Average specificity:  0.9244444444444445 +- 0.021180351389222064 \n",
            "Average balanced accuracy:  0.4986839266450917 +- 0.000981733278298091\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_b, model_b1, model_b2):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDiJR9KHJECj"
      },
      "source": [
        "Evaluate unbiased models on unbiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic5MOv2KRmPD",
        "outputId": "040e8bf1-9ef4-459f-a44c-0086ffd09d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6763754045307443 +- 0.1269043141505637 \n",
            "Average specificity:  0.5703703703703703 +- 0.1221121096402602 \n",
            "Average balanced accuracy:  0.6233728874505574 +- 0.004773879838228415\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PAK4dsWJECj"
      },
      "source": [
        "Evaluate unbiased models on a biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVG_q6YmRwxB",
        "outputId": "8aa3387a-0177-477b-b887-d684d9a66002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.5200391756089252 +- 0.08141336966739121 \n",
            "Average specificity:  0.6351515151515151 +- 0.0774237383604537 \n",
            "Average balanced accuracy:  0.5775953453802202 +- 0.020757880314016253\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "mean_sens = np.array(all_sens).mean()\n",
        "mean_spec = np.array(all_spec).mean()\n",
        "mean_score = np.array(all_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(all_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(all_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(all_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W22vxHJgFua0"
      },
      "source": [
        "**Biased Global Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFGXiSnF_SLi"
      },
      "outputs": [],
      "source": [
        "model_path = 'Pleural_effusion/Global_biased/Fully_global/'\n",
        "model_ser = torch.load(model_path + 'ppnet_chest/server_final_round_2_0.6157.pth').module.to(device)\n",
        "model_ser1 = torch.load(model_path + 'ppnet_chest_1/server_final_round_2_0.6164.pth').module.to(device)\n",
        "model_ser2 = torch.load(model_path + 'ppnet_chest_2/server_final_round_2_0.6157.pth').module.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXbulsm4JECk"
      },
      "source": [
        "Evaluate on a biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9hQmcB1yJcb",
        "outputId": "3fd0fec9-d616-4988-eebd-b08098224d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.0 +- 0.0 \n",
            "Average specificity:  0.9944444444444445 +- 0.005555555555555574 \n",
            "Average balanced accuracy:  0.49722222222222223 +- 0.002777777777777787\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_ser, model_ser1, model_ser2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjwCbkS_JECl"
      },
      "source": [
        "Evaluate on unbiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFCbPD1XyQYi",
        "outputId": "532ad0e2-3bd8-4d89-8c90-c09c4f505f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.0015102481121898597 +- 0.00151024811218986 \n",
            "Average specificity:  0.9987878787878789 +- 0.0012121212121211828 \n",
            "Average balanced accuracy:  0.5001490634500344 +- 0.00014906345003432947\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_ser, model_ser1, model_ser2):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IIEbrOWO4zP"
      },
      "source": [
        "## Cardiomegaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I_ppgBdSJECl"
      },
      "outputs": [],
      "source": [
        "data_path = 'cardiomegaly/'\n",
        "train_dir = data_path + 'train/'\n",
        "test_dir = data_path + 'test/'\n",
        "train_push_dir = data_path + 'push/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4kC-rwGPH4X"
      },
      "source": [
        "**Centralized Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data"
      ],
      "metadata": {
        "id": "JdpwFi0qaJhZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfkEy_LAaI8M"
      },
      "outputs": [],
      "source": [
        "# train set\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    train_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=False)\n",
        "# push set\n",
        "train_push_dataset = datasets.ImageFolder(\n",
        "    train_push_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]))\n",
        "train_push_loader = torch.utils.data.DataLoader(\n",
        "    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
        "    num_workers=2, pin_memory=False)\n",
        "# test set\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    test_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
        "    num_workers=2, pin_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmZF2SSfJECm"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnc8AsX5PH4x"
      },
      "outputs": [],
      "source": [
        "model = torch.load('Cardiomegaly/ppnet_chest/27nopush0.8263.pth')\n",
        "model_1 = torch.load('Cardiomegaly/ppnet_chest_1/26nopush0.8298.pth')\n",
        "model_2 = torch.load('Cardiomegaly/ppnet_chest_2/27nopush0.7810.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9mW7wrWaiHj",
        "outputId": "a3dcb503-e91c-4742-ac0a-48fe16db1a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torcheval/metrics/functional/classification/accuracy.py:275: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:230.)\n",
            "  num_correct = mask.new_zeros(num_classes).scatter_(0, target, mask, reduce=\"add\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6576543209876543 +- 0.03811777127693675 \n",
            "Average specificity:  0.8313271604938272 +- 0.02369769487238998 \n",
            "Average balanced accuracy:  0.7444907407407407 +- 0.00734329572811735\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model, model_1, model_2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loader, class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DrSNAH5Q44y"
      },
      "source": [
        "**Unbiased Local Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etHS0CrmJECo"
      },
      "source": [
        "Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ltqj8jcJECo"
      },
      "source": [
        "Please, first remove or rename the `client_0`, `client_1`, `client_2`, and `client_3` folders with pleural effusion data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CurgUhI9JECo"
      },
      "outputs": [],
      "source": [
        "dir_names = os.listdir(train_dir)\n",
        "for client in range(4): # 4 clients\n",
        "  os.mkdir(f'client_{client}')\n",
        "  os.mkdir(f'client_{client}/' + 'train/')\n",
        "  os.mkdir(f'client_{client}/' + 'push/')\n",
        "  os.mkdir(f'client_{client}/' + 'test/')\n",
        "  for class_name in dir_names:\n",
        "    os.mkdir(f'client_{client}/'+ 'train/' + class_name)\n",
        "    os.mkdir(f'client_{client}/'+ 'push/' + class_name)\n",
        "    os.mkdir(f'client_{client}/'+ 'test/' + class_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iba-5IfwJECp"
      },
      "outputs": [],
      "source": [
        "distribute_data(train_dir, seed, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cXdQboKiJECp"
      },
      "outputs": [],
      "source": [
        "distribute_data(train_push_dir, seed, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gTc-j7vlJECq"
      },
      "outputs": [],
      "source": [
        "distribute_data(test_dir, seed, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2iWYbDVjJECq"
      },
      "outputs": [],
      "source": [
        "num_clients = 4\n",
        "train_datasets, train_loaders = [],[]\n",
        "train_push_datasets, train_push_loaders = [],[]\n",
        "test_datasets, test_loaders = [],[]\n",
        "\n",
        "for client in range(num_clients):\n",
        "  # train set\n",
        "  train_dir = f'client_{client}/' + 'train/'\n",
        "  train_push_dir = f'client_{client}/' + 'push/'\n",
        "  test_dir = f'client_{client}/' + 'test/'\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(\n",
        "      train_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ]))\n",
        "  train_datasets.append(train_dataset)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  train_loaders.append(train_loader)\n",
        "\n",
        "  # push set\n",
        "  train_push_dataset = datasets.ImageFolder(\n",
        "      train_push_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "      ]))\n",
        "  train_push_datasets.append(train_push_dataset)\n",
        "\n",
        "  train_push_loader = torch.utils.data.DataLoader(\n",
        "      train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  train_push_loaders.append(train_push_loader)\n",
        "\n",
        "  # test set\n",
        "  test_dataset = datasets.ImageFolder(\n",
        "      test_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ]))\n",
        "  test_datasets.append(test_dataset)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset, batch_size=test_batch_size, shuffle=False,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  test_loaders.append(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbKZTn3hJECr"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Zmg6u00iQ6YL"
      },
      "outputs": [],
      "source": [
        "model_1 = torch.load('Local_1/29nopush0.7720.pth')\n",
        "model_2 = torch.load('Local_2/ppnet_chest/30nopush0.7716.pth')\n",
        "model_3 = torch.load('Local_3/ppnet_chest/29nopush0.7250.pth')\n",
        "model_4 = torch.load('Local_4/30nopush0.7554.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I--TM8RQ6YM",
        "outputId": "c4399dd7-25b4-4e4d-db42-7786cafbd6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torcheval/metrics/functional/classification/accuracy.py:275: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:230.)\n",
            "  num_correct = mask.new_zeros(num_classes).scatter_(0, target, mask, reduce=\"add\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6577777777777778 +- 0.03594596615551212 \n",
            "Average specificity:  0.7749742798353909 +- 0.015068965723075266 \n",
            "Average balanced accuracy:  0.7163760288065844 +- 0.010467782487058213\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3, model_4):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(4):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOSV05pfRs_j"
      },
      "source": [
        "**Unbiased Personalized Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGW-ZE_oJECw"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3s3Kn_qRy3k"
      },
      "outputs": [],
      "source": [
        "model_path = 'Global_good/ppnet_chest/'\n",
        "model_1 = torch.load(model_path + 'client_0_last_round_3.pth')\n",
        "model_2 = torch.load(model_path + 'client_1_last_round_3.pth')\n",
        "model_3 = torch.load(model_path + 'client_2_last_round_3.pth')\n",
        "model_4 = torch.load(model_path + 'client_3_last_round_3.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9kxfmSqRy31",
        "outputId": "9e55925b-0391-4885-ad70-0d7e86581b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6025 +- 0.08433409714082342 \n",
            "Average specificity:  0.6722350823045268 +- 0.05247839498021155 \n",
            "Average balanced accuracy:  0.6373675411522635 +- 0.0445382666534138\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3, model_4):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(4):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbeqXu05sM_n"
      },
      "source": [
        "**Unbiased Global Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqjLAEgL62Z5"
      },
      "outputs": [],
      "source": [
        "model_path = 'Global_good/Fully_global/'\n",
        "model_ser = torch.load(model_path + 'ppnet_chest/server_final_round_3_0.8621.pth').module.to(device)\n",
        "model_ser1 = torch.load(model_path + 'ppnet_chest_1/server_final_round_3_0.7501.pth').module.to(device)\n",
        "model_ser2 = torch.load(model_path + 'ppnet_chest_2/server_final_round_3_0.7957.pth').module.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1CV_nQsQZ58",
        "outputId": "7afaad11-9a25-4f04-8b87-a55e6b9b7486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6840740740740742 +- 0.07806326207141816 \n",
            "Average specificity:  0.7988168724279836 +- 0.0629793581337403 \n",
            "Average balanced accuracy:  0.7414454732510288 +- 0.007693561766996538\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_ser, model_ser1, model_ser2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loader, class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBj1akdReLS"
      },
      "source": [
        "**Biased Local Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk1l3plVJECs"
      },
      "source": [
        "Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkKvg4LAEtw6",
        "outputId": "7850347f-d4f8-45ad-95ea-7a107dee997f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6075\n",
            "750\n",
            "675\n"
          ]
        }
      ],
      "source": [
        "# add synthetic bias to the fourth client dataset\n",
        "\n",
        "num_client = 3\n",
        "unicode = '\\U0001F42D'\n",
        "bias_folder = 'positive'\n",
        "size = 35\n",
        "percent = 100\n",
        "adding_emoji(num_client, unicode, bias_folder, size, percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwg0Cp3KJECt"
      },
      "outputs": [],
      "source": [
        "num_clients = 4\n",
        "train_datasets, train_loaders = [],[]\n",
        "train_push_datasets, train_push_loaders = [],[]\n",
        "test_datasets, test_loaders = [],[]\n",
        "\n",
        "for client in range(num_clients):\n",
        "  # train set\n",
        "  train_dir = f'client_{client}/' + 'train/'\n",
        "  train_push_dir = f'client_{client}/' + 'push/'\n",
        "  test_dir = f'client_{client}/' + 'test/'\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(\n",
        "      train_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ]))\n",
        "  train_datasets.append(train_dataset)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  train_loaders.append(train_loader)\n",
        "\n",
        "  # push set\n",
        "  train_push_dataset = datasets.ImageFolder(\n",
        "      train_push_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "      ]))\n",
        "  train_push_datasets.append(train_push_dataset)\n",
        "\n",
        "  train_push_loader = torch.utils.data.DataLoader(\n",
        "      train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  train_push_loaders.append(train_push_loader)\n",
        "\n",
        "  # test set\n",
        "  test_dataset = datasets.ImageFolder(\n",
        "      test_dir,\n",
        "      transforms.Compose([\n",
        "          transforms.Resize(size=(img_size, img_size)),\n",
        "          transforms.ToTensor(),\n",
        "          normalize,\n",
        "      ]))\n",
        "  test_datasets.append(test_dataset)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset, batch_size=test_batch_size, shuffle=False,\n",
        "      num_workers=2, pin_memory=False)\n",
        "  test_loaders.append(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncWDYtyjJECt"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQLW_QMDRjwY"
      },
      "outputs": [],
      "source": [
        "model_b = torch.load('Local_4_biased/ppnet_chest/20_11push1.0000.pth')\n",
        "model_b1 = torch.load('Local_4_biased/ppnet_chest_1/20_11push1.0000.pth')\n",
        "model_b2 = torch.load('Local_4_biased/ppnet_chest_2/20_11push1.0000.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgOuVCBFJECu"
      },
      "source": [
        "Evaluate on a biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I61leNFAfmG7",
        "outputId": "3647a7f3-f6c9-4ec8-cf25-7e2ece68e23e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  1.0 +- 0.0 \n",
            "Average specificity:  1.0 +- 0.0 \n",
            "Average balanced accuracy:  1.0 +- 0.0\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_b, model_b1, model_b2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SV6VbJKJECv"
      },
      "source": [
        "Evaluate on unbiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8h00ADJf0mj",
        "outputId": "f04bb5c9-1644-4882-bf39-7ee474b2ce5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.0 +- 0.0 \n",
            "Average specificity:  1.0 +- 0.0 \n",
            "Average balanced accuracy:  0.5 +- 0.0\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_b, model_b1, model_b2):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF0laL0YSii7"
      },
      "source": [
        "**Biased Personalized Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UksNfb2kSSIq"
      },
      "outputs": [],
      "source": [
        "# unbiased clients\n",
        "model_path = 'Global_biased/ppnet_chest/'\n",
        "model_1 = torch.load(model_path + 'client_0_last_round_3_push0.8547.pth').module.to(device)\n",
        "model_2 = torch.load(model_path + 'client_1_last_round_3_push0.6343.pth')\n",
        "model_3 = torch.load(model_path + 'client_2_last_round_3_push0.7491.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pznl6gmWf7nw"
      },
      "outputs": [],
      "source": [
        "# biased client\n",
        "model = torch.load('Global_biased/ppnet_chest/client_3_last_round_3_push1.0000.pth').module.to(device)\n",
        "model1 = torch.load('Global_biased/ppnet_chest_1/client_3_last_round_3_push0.9221.pth').module.to(device)\n",
        "model2 = torch.load('Global_biased/ppnet_chest_2/client_3_last_round_3_push1.0000.pth').module.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO-sDRBHJECz"
      },
      "source": [
        "Evaluate biased model on biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gmmf3MCkWGE",
        "outputId": "32166a05-28cc-4eb7-f243-8b39f390a68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.7960493827160494 +- 0.2039506172839506 \n",
            "Average specificity  1.0 +- 0.0 \n",
            "Average balanced accuracy:  0.8980246913580247 +- 0.1019753086419753\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model, model1, model2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YljvoUzJEC0"
      },
      "source": [
        "Evaluate biased model on unbiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-ayJXb0kgVZ",
        "outputId": "fcfbb9cf-9c25-43e5-e9f5-a96f44f23f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.0 +- 0.0 \n",
            "Average specificity:  1.0 +- 0.0 \n",
            "Average balanced accuracy:  0.5 +- 0.0\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model, model1, model2):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WWdpYJrJEC1"
      },
      "source": [
        "Evaluate unbiased models on ubiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQmSUodcSqSN",
        "outputId": "110488f1-0ed3-4b21-9c15-51719f80b176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.5644444444444444 +- 0.10207101996103893 \n",
            "Average specificity:  0.7725880201188843 +- 0.08187770685561606 \n",
            "Average balanced accuracy:  0.6685162322816645 +- 0.02294206399914663\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4WH0UaoJEC2"
      },
      "source": [
        "Evaluate unbiased models on biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC8iQUsVSqSW",
        "outputId": "cccb45da-404f-4c07-dd6f-76ff01876593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.6059259259259259 +- 0.050296241749820426 \n",
            "Average specificity:  0.7721536351165982 +- 0.03692314704862767 \n",
            "Average balanced accuracy:  0.689039780521262 +- 0.01584169342006656\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_1, model_2, model_3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "mean_sens = np.array(all_sens).mean()\n",
        "mean_spec = np.array(all_spec).mean()\n",
        "mean_score = np.array(all_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(all_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(all_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(all_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlOp_d-xzzgM"
      },
      "source": [
        "**Biased Global Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w72vTJnxoy-K"
      },
      "outputs": [],
      "source": [
        "model_path = 'Global_biased/Fully_global/'\n",
        "model_ser = torch.load(model_path + 'ppnet_chest/server_final_round_3_0.4148.pth').module.to(device)\n",
        "model_ser1 = torch.load(model_path + 'ppnet_chest_1/server_final_round_3_0.8793.pth').module.to(device)\n",
        "model_ser2 = torch.load(model_path + 'ppnet_chest_2/server_final_round_3_0.8829.pth').module.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK-AE5CnJEC3"
      },
      "source": [
        "Evaluate on biased test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEi0r2JDRguN",
        "outputId": "c9550019-040c-4eb9-dc88-6af3294ffa44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.45629629629629626 +- 0.2750652747072852 \n",
            "Average specificity:  0.7743484224965705 +- 0.2224664718934485 \n",
            "Average balanced accuracy:  0.6153223593964334 +- 0.04268366692388824\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_ser, model_ser1, model_ser2):\n",
        "  acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[3], class_specific=True)\n",
        "  fin_sens.append(sens)\n",
        "  fin_spec.append(spec)\n",
        "  fin_score.append(score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbDqhElbJEC4"
      },
      "source": [
        "Evaluate on unbiased test sets and average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK0yKj-Ewddk",
        "outputId": "08cb321f-b15a-4f88-b14a-1814159f30e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sensitivity:  0.3422222222222222 +- 0.29566640865944127 \n",
            "Average specificity:  0.7748742569730224 +- 0.22286439490541643 \n",
            "Average balanced accuracy:  0.5585482395976223 +- 0.03685498222803084\n"
          ]
        }
      ],
      "source": [
        "fin_sens = []\n",
        "fin_spec = []\n",
        "fin_score = []\n",
        "for j in (model_ser, model_ser1, model_ser2):\n",
        "  all_sens = []\n",
        "  all_spec = []\n",
        "  all_score = []\n",
        "  for i in range(3):\n",
        "    acc, f1, acc_multi, sens, spec, score = evaluate(j, test_loaders[i], class_specific=True)\n",
        "    all_sens.append(sens)\n",
        "    all_spec.append(spec)\n",
        "    all_score.append(score)\n",
        "\n",
        "  mean_sens = np.array(all_sens).mean()\n",
        "  mean_spec = np.array(all_spec).mean()\n",
        "  mean_score = np.array(all_score).mean()\n",
        "  fin_sens.append(mean_sens)\n",
        "  fin_spec.append(mean_spec)\n",
        "  fin_score.append(mean_score)\n",
        "\n",
        "mean_sens = np.array(fin_sens).mean()\n",
        "mean_spec = np.array(fin_spec).mean()\n",
        "mean_score = np.array(fin_score).mean()\n",
        "print('Average sensitivity: ', mean_sens, '+-', st.sem(fin_sens),\n",
        "      '\\nAverage specificity: ', mean_spec, '+-', st.sem(fin_spec),\n",
        "      '\\nAverage balanced accuracy: ', mean_score, '+-', st.sem(fin_score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}